{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"sparkSqlSession\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"first_name\", StringType(), True),\n",
    "    StructField(\"last_name\", StringType(), True),\n",
    "    StructField(\"email\", StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True),\n",
    "    StructField(\"employment_date\", DateType(), True)\n",
    "])\n",
    "\n",
    "df = spark.read.load(\"sparkSqlData.csv\", format=\"csv\", sep=\",\", schema=schema, header=\"true\")\n",
    "\n",
    "df.createOrReplaceTempView(\"df\")\n",
    "\n",
    "dfPd = df.toPandas()\n",
    "    \n",
    "df.show(5)\n",
    "\n",
    "dfPd.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pyspark.sql\n",
    "df.describe(\"salary\").show()\n",
    "\n",
    "#pandas\n",
    "dfPd.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pyspark.sql\n",
    "df.select(\"first_name\", \"last_name\").show(5)\n",
    "\n",
    "#sql\n",
    "spark.sql(\"SELECT first_name, last_name \\\n",
    "           FROM df\").show(5)\n",
    "\n",
    "#pandas\n",
    "dfPd.loc[:,[\"first_name\", \"last_name\"]].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pyspark.sql\n",
    "df.select(\"first_name\", \"last_name\", \"salary\").where(\"salary > 90000\").show(5)\n",
    "\n",
    "#sql\n",
    "spark.sql(\"SELECT first_name, last_name, salary \\\n",
    "           FROM df \\\n",
    "           WHERE salary > 90000\").show(5)\n",
    "\n",
    "#pandas\n",
    "dfPd.loc[dfPd[\"salary\"] > 90000, [\"first_name\", \"last_name\", \"salary\"]].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Order by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pyspark.sql\n",
    "df.select(\"first_name\", \"Last_name\", \"salary\").orderBy(desc(\"salary\")).show(5)\n",
    "#df.select(\"first_name\", \"Last_name\", \"salary\").orderBy(df.salary.desc()).show(5)\n",
    "#df.select(\"first_name\", \"Last_name\", \"salary\").orderBy(\"salary\", ascending=False).show(5)\n",
    "\n",
    "#sql\n",
    "spark.sql(\"SELECT first_name, last_name, salary \\\n",
    "           FROM df \\\n",
    "           ORDER BY salary DESC\").show(5)\n",
    "\n",
    "#pandas\n",
    "dfPd.loc[:, [\"first_name\", \"last_name\", \"salary\"]].sort_values(by=\"salary\", ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regexp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pyspark.sql\n",
    "df.filter(df.email.rlike('google*')).show(truncate=False, n=5)\n",
    "\n",
    "#sql\n",
    "spark.sql(\"SELECT * \\\n",
    "           FROM df \\\n",
    "           WHERE email REGEXP 'google*'\").show(truncate=False, n=5)\n",
    "\n",
    "#pandas\n",
    "dfPd.loc[dfPd.loc[:, \"email\"].str.contains('google*')].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pyspark.sql\n",
    "df.filter(df.first_name.rlike('^[AB]|[ab]$')).show(5)\n",
    "\n",
    "#sql\n",
    "spark.sql(\"SELECT * \\\n",
    "           FROM df \\\n",
    "           WHERE first_name REGEXP '^[AB]|[ab]$'\").show(5)\n",
    "\n",
    "#pandas\n",
    "dfPd.loc[dfPd.loc[:, \"first_name\"].str.contains('^[AB]|[ab]$')].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## String length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pyspark.sql\n",
    "df.select(\"first_name\", length(\"first_name\").alias(\"first_name_length\")) \\\n",
    "  .orderBy(desc(\"first_name_length\"), asc(\"first_name\")).show(5)\n",
    "\n",
    "#sql\n",
    "spark.sql(\"SELECT first_name, length(first_name) AS first_name_length \\\n",
    "           FROM df \\\n",
    "           ORDER BY first_name_length DESC, first_name ASC\").show(5)\n",
    "\n",
    "#pandas\n",
    "dfPd.assign(first_name_length=dfPd.loc[:, \"first_name\"].str.len()) \\\n",
    "    .loc[:, [\"first_name\", \"first_name_length\"]] \\\n",
    "    .sort_values(by=[\"first_name_length\", \"first_name\"], ascending=[False, True]).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pyspark.sql\n",
    "df.select(\"last_name\", length(\"last_name\").alias(\"last_name_length\")) \\\n",
    "  .orderBy(asc(\"last_name_length\"), asc(\"last_name\")).show(5)\n",
    "\n",
    "#sql\n",
    "shortestLastName = spark.sql(\"SELECT last_name, length(last_name) AS last_name_length \\\n",
    "                              FROM df \\\n",
    "                              ORDER BY last_name_length ASC, last_name ASC\")\n",
    "\n",
    "shortestLastName.show(5)\n",
    "\n",
    "shortestLastName = shortestLastName.take(1)[0][0]\n",
    "\n",
    "print(\"The shortest last name alphabetically ordered is:\", shortestLastName)\n",
    "\n",
    "#pandas\n",
    "dfPd.assign(last_name_length=dfPd.loc[:, \"last_name\"].str.len()) \\\n",
    "    .loc[:, [\"last_name\", \"last_name_length\"]] \\\n",
    "    .sort_values(by=[\"last_name_length\", \"last_name\"], ascending=[True, True]).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Date range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pyspark.sql\n",
    "df.select(\"last_name\", \"employment_date\").filter(df[\"employment_date\"].between('2018-12-01', '2018-12-20')).show(5)\n",
    "\n",
    "#sql\n",
    "spark.sql(\"SELECT last_name, employment_date \\\n",
    "           FROM df \\\n",
    "           WHERE employment_date BETWEEN '2018-12-01' AND '2018-12-20'\").show(5)\n",
    "\n",
    "#pandas\n",
    "dfPd[\"employment_date\"] = pd.to_datetime(dfPd[\"employment_date\"])\n",
    "\n",
    "dfPd.loc[dfPd[\"employment_date\"].between('2018-12-01', '2018-12-20'), [\"last_name\", \"employment_date\"]].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pyspark.sql\n",
    "df.select(abs(df[\"salary\"]/(df.select(sum(\"salary\")).take(1)[0][0])).alias(\"salary_normalized\")).show(5)\n",
    "\n",
    "#sql\n",
    "spark.sql(\"SELECT salary/(SELECT sum(salary) FROM df) AS salary_normalized \\\n",
    "           FROM df\").show(5)\n",
    "\n",
    "#pandas\n",
    "dfPd.assign(salary_normalized=dfPd[\"salary\"]/dfPd[\"salary\"].sum()).loc[:, \"salary_normalized\"].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfJoin = spark.read.load(\"sparkSqlData2.csv\", format=\"csv\", sep=\",\", inferSchema=True, header=\"true\")\n",
    "\n",
    "dfJoin.createOrReplaceTempView(\"dfJoin\")\n",
    "\n",
    "dfJoinPd = dfJoin.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pyspark.sql\n",
    "df.join(dfJoin, on=\"id\").show(5)\n",
    "\n",
    "#sql\n",
    "spark.sql(\"SELECT * FROM df JOIN dfJoin USING (id)\").show(5)\n",
    "\n",
    "#pandas\n",
    "dfPd.merge(dfJoinPd, on=\"id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second hightest salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pyspark.sql\n",
    "df.filter(~df.salary.isin(df.select(max(\"salary\")).take(1)[0][0])).select(max(\"salary\") \\\n",
    "  .alias(\"secondHighestSalary\")).show(5)\n",
    "\n",
    "#sql\n",
    "spark.sql(\"SELECT max(salary) AS secondHighestSalary FROM df WHERE salary != (SELECT max(salary) FROM df)\").show(5)\n",
    "\n",
    "#pandas\n",
    "dfPd[~dfPd[\"salary\"].isin([dfPd[\"salary\"].max()])].loc[:, \"salary\"].max()\n",
    "#dfPd.where(dfPd[\"salary\"] != dfPd[\"salary\"].max()).loc[:, \"salary\"].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nth highest salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "#pyspark.sql\n",
    "df.select(\"salary\", dense_rank().over(Window.orderBy(desc(\"salary\"))).alias(\"rank\")) \\\n",
    "  .select(\"salary\").where(\"rank = 5\").show()\n",
    "\n",
    "#sql\n",
    "spark.sql(\"SELECT salary \\\n",
    "           FROM (SELECT salary, DENSE_RANK() OVER (ORDER BY salary DESC) AS rank FROM df) dfTemp \\\n",
    "           WHERE dfTemp.rank = 5\").show(5)\n",
    "#pandas\n",
    "dfTemp = dfPd.assign(rank = dfPd[\"salary\"].rank(method='dense',ascending=False))\n",
    "dfTemp.loc[dfTemp[\"rank\"]==5, \"salary\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elapsed dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pyspark.sql\n",
    "df.select(\"last_name\", \"employment_date\", datediff(current_timestamp(), \"employment_date\") \\\n",
    "          .alias(\"elapsedDatesSinceHired\")).orderBy(desc(\"employment_date\")) \\\n",
    "          .filter(datediff(current_timestamp(), \"employment_date\").between(1, 500)).show()\n",
    "#sql\n",
    "spark.sql(\"SELECT last_name, employment_date, DATEDIFF(current_date(), employment_date) AS elapsedDatesSinceHired \\\n",
    "           FROM df WHERE (DATEDIFF(current_date(), employment_date)) BETWEEN 1 AND 500 \\\n",
    "           ORDER BY employment_date DESC\").show()\n",
    "#pandas\n",
    "dfPd.assign(elapsedDatesSinceHired = ((pd.Timestamp.now() - dfPd[\"employment_date\"])).dt.days) \\\n",
    "    .loc[((pd.Timestamp.now() - dfPd[\"employment_date\"])).dt.days.between(1, 500) \\\n",
    "         ,[\"last_name\", \"employment_date\", \"elapsedDatesSinceHired\"]] \\\n",
    "    .sort_values(by=\"employment_date\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
